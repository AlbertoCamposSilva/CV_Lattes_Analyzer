{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93b9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from Lattes import Lattes\n",
    "from Indicadores import Indicadores\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from datetime import date, datetime\n",
    "import pandas, collections\n",
    "import urllib.parse\n",
    "import uncurl\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import time\n",
    "import re\n",
    "import logging as logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c07e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarScrape():\n",
    "    def __init__(self):\n",
    "        self.page = None\n",
    "        self.last_url = None\n",
    "        self.last_time = time.time()\n",
    "        self.min_time_between_scrape = int(45)\n",
    "        self.header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "        self.session = requests.Session()\n",
    "        pass\n",
    "\n",
    "    def search(self, \n",
    "               query=None, \n",
    "               year_lo=None, \n",
    "               year_hi=None, \n",
    "               title_only=False, \n",
    "               publication_string=None, \n",
    "               author_string=None, \n",
    "               include_citations=True, \n",
    "               include_patents=True):\n",
    "        \n",
    "        url = self.get_url(query, \n",
    "                           year_lo, \n",
    "                           year_hi, \n",
    "                           title_only, \n",
    "                           publication_string, \n",
    "                           author_string, \n",
    "                           include_citations, \n",
    "                           include_patents)\n",
    "        while True:\n",
    "            wait_time = self.min_time_between_scrape - (time.time() - self.last_time)\n",
    "            if wait_time > 0:\n",
    "                logger.info(\"Delaying search by {} seconds to avoid bot detection.\".format(wait_time))\n",
    "                time.sleep(wait_time)\n",
    "            self.last_time = time.time()\n",
    "            logger.info(\"SCHOLARSCRAPE: \" + url)\n",
    "            self.page = BeautifulSoup(self.session.get(url, headers=self.header).text, 'html.parser')\n",
    "            self.last_url = url\n",
    "\n",
    "            if \"Our systems have detected unusual traffic from your computer network\" in str(self.page):\n",
    "                raise BotDetectionException(\"Google has blocked this computer for a short time because it has detected this scraping script.\")\n",
    "\n",
    "            return\n",
    "\n",
    "    def get_url(self, \n",
    "                query=None, \n",
    "                year_lo=None, \n",
    "                year_hi=None, \n",
    "                title_only=False, \n",
    "                publication_string=None, \n",
    "                author_string=None, \n",
    "                include_citations=True, \n",
    "                include_patents=True):\n",
    "        \n",
    "        base_url = \"https://scholar.google.com.au/scholar?\"\n",
    "        url = base_url + \"as_q=\" + urllib.parse.quote(query)\n",
    "\n",
    "        if year_lo is not None and bool(re.match(r'.*([1-3][0-9]{3})', str(year_lo))):\n",
    "            url += \"&as_ylo=\" + str(year_lo)\n",
    "\n",
    "        if year_hi is not None and bool(re.match(r'.*([1-3][0-9]{3})', str(year_hi))):\n",
    "            url += \"&as_yhi=\" + str(year_hi)\n",
    "\n",
    "        if title_only:\n",
    "            url += \"&as_yhi=title\"\n",
    "        else:\n",
    "            url += \"&as_yhi=any\"\n",
    "\n",
    "        if publication_string is not None:\n",
    "            url += \"&as_publication=\" + urllib.parse.quote('\"' + str(publication_string) + '\"')\n",
    "\n",
    "        if author_string is not None:\n",
    "            url += \"&as_sauthors=\" + urllib.parse.quote('\"' + str(author_string) + '\"')\n",
    "\n",
    "        if include_citations:\n",
    "            url += \"&as_vis=0\"\n",
    "        else:\n",
    "            url += \"&as_vis=1\"\n",
    "\n",
    "        if include_patents:\n",
    "            url += \"&as_sdt=0\"\n",
    "        else:\n",
    "            url += \"&as_sdt=1\"\n",
    "\n",
    "        return url\n",
    "\n",
    "    def get_results_count(self):\n",
    "        e = self.page.findAll(\"div\", {\"class\": \"gs_ab_mdw\"})\n",
    "        try:\n",
    "            item = e[1].text.strip()\n",
    "        except IndexError as ex:\n",
    "            if \"Our systems have detected unusual traffic from your computer network\" in str(self.page):\n",
    "                raise BotDetectionException(\"Google has blocked this computer for a short time because it has detected this scraping script.\")\n",
    "            else:\n",
    "                raise ex\n",
    "\n",
    "        if self.has_numbers(item):\n",
    "            return self.get_results_count_from_soup_string(item)\n",
    "        for item in e:\n",
    "            item = item.text.strip()\n",
    "            if self.has_numbers(item):\n",
    "                return self.get_results_count_from_soup_string(item)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_results_count_from_soup_string(element):\n",
    "        if \"About\" in element:\n",
    "            num = element.split(\" \")[1].strip().replace(\",\",\"\")\n",
    "        else:\n",
    "            num = element.split(\" \")[0].strip().replace(\",\",\"\")\n",
    "        return num\n",
    "\n",
    "    @staticmethod\n",
    "    def has_numbers(input_string):\n",
    "        return any(char.isdigit() for char in input_string)\n",
    "\n",
    "\n",
    "class BotDetectionException(Exception):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1a565a",
   "metadata": {},
   "outputs": [
    {
     "ename": "BotDetectionException",
     "evalue": "Google has blocked this computer for a short time because it has detected this scraping script.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBotDetectionException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14244/2087030489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScholarScrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     s.search(**{\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;34m\"query\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"\\\"policy shaping\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# \"publication_string\":\"JMLR\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14244/977887595.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, query, year_lo, year_hi, title_only, publication_string, author_string, include_citations, include_patents)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Our systems have detected unusual traffic from your computer network\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mBotDetectionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Google has blocked this computer for a short time because it has detected this scraping script.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBotDetectionException\u001b[0m: Google has blocked this computer for a short time because it has detected this scraping script."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    s = ScholarScrape()\n",
    "    s.search(**{\n",
    "        \"query\":\"\\\"policy shaping\\\"\",\n",
    "        # \"publication_string\":\"JMLR\",\n",
    "        \"author_string\": \"gilboa\",\n",
    "        \"year_lo\": \"1995\",\n",
    "        \"year_hi\": \"2005\",\n",
    "\n",
    "    })\n",
    "    x = s.get_results_count()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1afe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
